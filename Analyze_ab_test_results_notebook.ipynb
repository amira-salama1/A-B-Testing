{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze A/B Test Results\n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "- [Introduction](#intro)\n",
    "- [Part I - Probability](#probability)\n",
    "- [Part II - A/B Test](#ab_test)\n",
    "- [Part III - Regression](#regression)\n",
    "\n",
    "\n",
    "<a id='intro'></a>\n",
    "### Introduction\n",
    "\n",
    "A/B tests are very commonly performed by data analysts and data scientists.  It is important that you get some practice working with the difficulties of these \n",
    "\n",
    "For this project **The goal** is to understand the results of an A/B test run by an e-commerce website, working through this notebook to help the company understand if they should implement the new page, keep the old page, or perhaps run the experiment longer to make their decision.\n",
    "\n",
    "<a id='probability'></a>\n",
    "#### Part I - Probability\n",
    "\n",
    "To get started, let's import our libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#We are setting the seed to be 42\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>851104</td>\n",
       "      <td>2017-01-21 22:11:48.556739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>804228</td>\n",
       "      <td>2017-01-12 08:01:45.159739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>661590</td>\n",
       "      <td>2017-01-11 16:55:06.154213</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853541</td>\n",
       "      <td>2017-01-08 18:28:03.143765</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864975</td>\n",
       "      <td>2017-01-21 01:52:26.210827</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                   timestamp      group landing_page  converted\n",
       "0   851104  2017-01-21 22:11:48.556739    control     old_page          0\n",
       "1   804228  2017-01-12 08:01:45.159739    control     old_page          0\n",
       "2   661590  2017-01-11 16:55:06.154213  treatment     new_page          0\n",
       "3   853541  2017-01-08 18:28:03.143765  treatment     new_page          0\n",
       "4   864975  2017-01-21 01:52:26.210827    control     old_page          1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the dataframe and printing the first 5 rows \n",
    "df = pd.read_csv(r'\\\\ab_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "294478"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The Number of rows in the DF\n",
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. The number of unique users in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290584"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The Number of unique users id in the DF\n",
    "df.user_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "d. The proportion of users converted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11965919355605512"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# proportion of users converted\n",
    "df.query('converted == 1').shape[0]/df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "e. The number of times the `new_page` and `treatment` don't line up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3893"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The number of times the new_page and treatment don't line up.\n",
    "df.query(\"landing_page == 'new_page' and group == 'control'\").shape[0] + df.query(\"landing_page == 'old_page' and group == 'treatment'\").shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f. Do any of the rows have missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id         False\n",
       "timestamp       False\n",
       "group           False\n",
       "landing_page    False\n",
       "converted       False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking if the df has any missing values\n",
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` For the rows where **treatment** is not aligned with **new_page** or **control** is not aligned with **old_page**, we cannot be sure if this row truly received the new or old page. \n",
    "\n",
    "a.creating a new dataset that meets the specifications \n",
    "Store your new dataframe in **df2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the rows where treatment doesn't match with new_page or \n",
    "#control doesn't match with old_page then save clean DataFrame in to df2\n",
    "df_treatment = df.query(\"landing_page == 'old_page' and group == 'treatment'\").index\n",
    "df_control   = df.query(\"landing_page == 'new_page' and group == 'control'\").index\n",
    "total_drop_index = df_treatment.append(df_control)\n",
    "df2 = df.drop(total_drop_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Double Check all of the correct rows were removed - this should be 0\n",
    "df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. How many unique **user_id**s are in **df2**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290584"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The Number of unique users id in the DF2\n",
    "df2.user_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2893    773192\n",
       "Name: user_id, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting the duplicated user_id in DF2\n",
    "df2[df2.user_id.duplicated()].user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2893</th>\n",
       "      <td>773192</td>\n",
       "      <td>2017-01-14 02:55:59.590927</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id                   timestamp      group landing_page  converted\n",
       "2893   773192  2017-01-14 02:55:59.590927  treatment     new_page          0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# row information for the repeat user_id\n",
    "df2[df2.user_id.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing **one** of the rows with a duplicate **user_id**, but keeping your dataframe as **df2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the duplicate record\n",
    "df2.drop(df2[df2.user_id.duplicated()].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test\n",
    "df2.user_id.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. What is the probability of an individual converting regardless of the page they receive?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11959708724499628"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the probability that an individual converted regardless of the page they receive\n",
    "df2.query('converted == 1').shape[0]/df2.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Given that an individual was in the `control` group, what is the probability they converted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1203863045004612"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the probability that an individual in the control group converted\n",
    "control_p = df2.query('converted == 1 and group == \"control\"').shape[0]/df2.query('group == \"control\"').shape[0]\n",
    "control_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Given that an individual was in the `treatment` group, what is the probability they converted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11880806551510564"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the probability that an individual in the treatment group converted\n",
    "treatment_p = df2.query('converted == 1 and group == \"treatment\"').shape[0]/df2.query('group == \"treatment\"').shape[0]\n",
    "treatment_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. What is the probability that an individual received the new page?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5000619442226688"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the probability that an individual received the new page\n",
    "df2.query('landing_page == \"new_page\"').shape[0]/df2.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. Considering the results: **Explanation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From the results of the probability analysis, the New_page conversion rate (0.119) is lower than the Old_page conversion rate (0.12) with a very little difference, also there's an equal opportunity that an individual in each group can receive each page.\n",
    "These results suggest staying with the Old_page, However there's not enough evidence to say that the new treatment page leads to less conversion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ab_test'></a>\n",
    "### Part II - A/B Test\n",
    "\n",
    "Notice that because of the time stamp associated with each event, you could technically run a hypothesis test continuously as each observation was observed.  \n",
    "\n",
    "However, then the hard question is do you stop as soon as one page is considered significantly better than another or does it need to happen consistently for a certain amount of time?  How long do you run to render a decision that neither page is better than another?  \n",
    "\n",
    "These questions are the difficult parts associated with A/B tests in general.  \n",
    "\n",
    "\n",
    "`1.` For now, consider you need to make the decision just based on all the data provided.  If you want to assume that the old page is better unless the new page proves to be definitely better at a Type I error rate of 5%, \n",
    "- **Hypothesis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center\"> 1. $H_{0}$ : ($P_{new}$ - $P_{old}$ <= 0) </div>\n",
    "<div style=\"text-align: center\"> 2. $H_{1}$ : ($P_{new}$ - $P_{old}$ > 0)  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` Assume under the null hypothesis, $p_{new}$ and $p_{old}$ both have \"true\" success rates equal to the **converted** success rate regardless of page - that is $p_{new}$ and $p_{old}$ are equal. Furthermore, assume they are equal to the **converted** rate in **ab_data.csv** regardless of the page. <br><br>\n",
    "\n",
    "Using a sample size for each page equal to the ones in **ab_data.csv**.  <br><br>\n",
    "\n",
    "Perform the sampling distribution for the difference in **converted** between the two pages over 10,000 iterations of calculating an estimate from the null.  <br><br>\n",
    "\n",
    "Use the cells below to provide the necessary parts of this simulation.  If this doesn't make complete sense right now, don't worry - you are going to work through the problems below to complete this problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. What is the **convert rate** for $p_{new}$ under the null? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11959708724499628"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assumnig that the P_new and P_old have equal convertion rate\n",
    "# conversion rate for new page\n",
    "p_new = df2[\"converted\"].mean()\n",
    "p_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. What is the **convert rate** for $p_{old}$ under the null? <br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11959708724499628"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conversion rate for old page same as the p_new under the null hypothesis\n",
    "p_old = df2[\"converted\"].mean()\n",
    "p_old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. What is $n_{new}$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145310"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of individuals in the treatment group\n",
    "n_new = df2.query('group == \"treatment\"').shape[0]\n",
    "n_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. What is $n_{old}$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145274"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of individuals in the control group\n",
    "n_old = df2.query('group == \"control\"').shape[0]\n",
    "n_old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. Simulate $n_{new}$ transactions with a convert rate of $p_{new}$ under the null.  Store these $n_{new}$ 1's and 0's in **new_page_converted**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simulating n_new transactions with conversion probability of P_new\n",
    "new_page_converted = np.random.choice([0,1], size = n_new, p=[(1-p_new), p_new])\n",
    "new_page_converted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f. Simulate $n_{old}$ transactions with a convert rate of $p_{old}$ under the null.  Store these $n_{old}$ 1's and 0's in **old_page_converted**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simulating n_old transactions with conversion probability of P_old\n",
    "old_page_converted = np.random.choice([0,1], size = n_old, p=[(1-p_old), p_old])\n",
    "old_page_converted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g. Find $p_{new}$ - $p_{old}$ for your simulated values from part (e) and (f)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0009201652436779811"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_page_converted.mean() - old_page_converted.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h. Simulate 10,000 $p_{new}$ - $p_{old}$ values using this same process similarly to the one you calculated in parts **a. through g.** above.  Store all 10,000 values in a numpy array called **p_diffs**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating 10,000  𝑝𝑛𝑒𝑤  -  𝑝𝑜𝑙𝑑  values using this same process as before and storing them in a P_diffs list \n",
    "p_diffs = []\n",
    "for _ in range (10000):\n",
    "    new_page_converted = np.random.choice([0,1], n_new, p=[(1-p_new),p_new])\n",
    "    old_page_converted = np.random.choice([0,1], n_old, p=[(1-p_old),p_old])\n",
    "    p_diff = (new_page_converted.mean() - old_page_converted.mean())\n",
    "    p_diffs.append(p_diff)\n",
    "# creating a numpy array for p_diffs\n",
    "p_diffs = np.array(p_diffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i. Plot a histogram of the **p_diffs**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting a histogram of the P_diffs \n",
    "plt.hist(p_diffs)\n",
    "plt.title('Distribution of P_new and P_old proportions')\n",
    "plt.xlabel('P_new - P_old');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The histogram seems to be normally distributed**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "j. What proportion of the **p_diffs** are greater than the actual difference observed in **ab_data.csv**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute observed difference in the actual means of the control and treatment \n",
    "obs_diff = treatment_p -control_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.904"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculating the p-value\n",
    "p =(p_diffs > obs_diff).mean()\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k. **explanation:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In Part (j), I've calculated the P-value (0.9081) which is the probability of observing the statistic (or one more extreme in favor of the alternative) if the null hypothesis is true, the high value suggest the the null Hypothesis is true and our statistic came from the null hypothesis, so we fail to reject the null hypothesis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l. We could also use a built-in to achieve similar results.  Though using the built-in might be easier to code, the above portions are a walkthrough of the ideas that are critical to correctly thinking about statistical significance. Fill in the below to calculate the number of conversions for each page, as well as the number of individuals who received each page. Let `n_old` and `n_new` refer the the number of rows associated with the old page and new pages, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing statsmodels\n",
    "import statsmodels.api as sm\n",
    "\n",
    "#setting variables for the stats model\n",
    "convert_old = df2.query('group == \"control\" and converted == 1').shape[0]\n",
    "convert_new = df2.query('group == \"treatment\" and converted == 1').shape[0]\n",
    "n_old = df2.query('group == \"control\"').shape[0]\n",
    "n_new = df2.query('group == \"treatment\"').shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "m. Now use `stats.proportions_ztest` to compute your test statistic and p-value.  [Here](http://knowledgetack.com/python/statsmodels/proportions_ztest/) is a helpful link on using the built in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.3109241984234394, 0.9050583127590245)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# computing the z-score and p-value through python's library statsmodels\n",
    "z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative= 'smaller')\n",
    "z_score, p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n. What do the z-score and p-value you computed in the previous question mean for the conversion rates of the old and new pages?  Do they agree with the findings in parts **j.** and **k.**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9050583127590245\n",
      "1.6448536269514722\n"
     ]
    }
   ],
   "source": [
    "# Getting the critical value of Z-score to compare it to our results.\n",
    "from scipy.stats import norm\n",
    "\n",
    "# The percentage of the significance of z-score is\n",
    "print(norm.cdf(z_score))\n",
    "\n",
    "# What is the critical value at 95% confidence at type I error\n",
    "print(norm.ppf(1-0.05))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Z- Score :** is 1.31 standard deviations above the mean of the population and below the critical value of 1.64 which means that we fail to reject the Null hypothesis also **p_value** is 0.91 which is consistent with the results in parts j and k, the P-value is higher than type 1 error rate which means we fail to reject the Null hypoythesis again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='regression'></a>\n",
    "### Part III - A regression approach\n",
    "\n",
    "`1.` In this final part, you will see that the result acheived in the previous A/B test can also be acheived by performing regression.<br><br>\n",
    "\n",
    "a. Since each row is either a conversion or no conversion, what type of regression should you be performing in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We should be performing a logistic regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. The goal is to use **statsmodels** to fit the regression model you specified in part **a.** to see if there is a significant difference in conversion based on which page a customer receives.  However,  first we need to create a column for the intercept, and create a dummy variable column for which page each user received.  Add an **intercept** column, as well as an **ab_page** column, which is 1 when an individual receives the **treatment** and 0 if **control**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "      <th>intercept</th>\n",
       "      <th>ab_page</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>851104</td>\n",
       "      <td>2017-01-21 22:11:48.556739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>804228</td>\n",
       "      <td>2017-01-12 08:01:45.159739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>661590</td>\n",
       "      <td>2017-01-11 16:55:06.154213</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853541</td>\n",
       "      <td>2017-01-08 18:28:03.143765</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864975</td>\n",
       "      <td>2017-01-21 01:52:26.210827</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                   timestamp      group landing_page  converted  \\\n",
       "0   851104  2017-01-21 22:11:48.556739    control     old_page          0   \n",
       "1   804228  2017-01-12 08:01:45.159739    control     old_page          0   \n",
       "2   661590  2017-01-11 16:55:06.154213  treatment     new_page          0   \n",
       "3   853541  2017-01-08 18:28:03.143765  treatment     new_page          0   \n",
       "4   864975  2017-01-21 01:52:26.210827    control     old_page          1   \n",
       "\n",
       "   intercept  ab_page  \n",
       "0          1        0  \n",
       "1          1        0  \n",
       "2          1        1  \n",
       "3          1        1  \n",
       "4          1        0  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#adding an intercept column to df2 \n",
    "df2['intercept'] = 1\n",
    "# adding an ab_page column as a dummy variable for the categorical column (landing_page)\n",
    "df2['ab_page']   = pd.get_dummies(df2['landing_page'])['new_page']\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Use **statsmodels** to import your regression model.  Instantiate the model, and fit the model using the two columns you created in part **b.** to predict whether or not an individual converts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366118\n",
      "         Iterations 6\n"
     ]
    }
   ],
   "source": [
    "# using statsmodels to import logistic regression model then fitting the model\n",
    "log_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']])\n",
    "results = log_mod.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Provide the summary of the model below, and use it as necessary to answer the following questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>converted</td>    <th>  No. Observations:  </th>   <td>290584</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td>290582</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>     1</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Wed, 13 Jan 2021</td> <th>  Pseudo R-squ.:     </th>  <td>8.077e-06</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>22:51:21</td>     <th>  Log-Likelihood:    </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>   <td>0.1899</td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>   -1.9888</td> <td>    0.008</td> <td> -246.669</td> <td> 0.000</td> <td>   -2.005</td> <td>   -1.973</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ab_page</th>   <td>   -0.0150</td> <td>    0.011</td> <td>   -1.311</td> <td> 0.190</td> <td>   -0.037</td> <td>    0.007</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:              converted   No. Observations:               290584\n",
       "Model:                          Logit   Df Residuals:                   290582\n",
       "Method:                           MLE   Df Model:                            1\n",
       "Date:                Wed, 13 Jan 2021   Pseudo R-squ.:               8.077e-06\n",
       "Time:                        22:51:21   Log-Likelihood:            -1.0639e+05\n",
       "converged:                       True   LL-Null:                   -1.0639e+05\n",
       "Covariance Type:            nonrobust   LLR p-value:                    0.1899\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept     -1.9888      0.008   -246.669      0.000      -2.005      -1.973\n",
       "ab_page       -0.0150      0.011     -1.311      0.190      -0.037       0.007\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print results of the logistic regression model\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intercept    0.136863\n",
       "ab_page      0.985123\n",
       "dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exponentiating the results summary to interpret the values\n",
    "np.exp(results.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation: if a user uses the new page he or she is 0.985 times likely to convert, which is still a very low value (less than 1)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. What is the p-value associated with **ab_page**? Why does it differ from the value you found in **Part II**?<br><br>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P-value : 0.190\n",
    "it differs from the previous value in part II because the hypothsis here is different\n",
    "**Null and alternative hypotheses associated with the logistic regression model is :\n",
    "$$ H_0: p_{new} = p_{old} $$$$ H_1: p_{new} \\neq p_{old} $$**\n",
    "\n",
    "**The Null and alternative hypothesis associated with th A/B testing in Part II was one sided "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It's useful to discuss adding new variable (exploratory) to the regression model because we're interested in understanding the relevance/importance of each variable.\n",
    "Adding a variable to a logistic regression model will cause a bias reduction in the coefficients estimates but an increase in their variances. Since you never know what are the real relevant variables, you need to balance this bias-variance trade-off.\n",
    "[Resource link](https://stats.stackexchange.com/questions/304707/costs-and-benefits-of-adding-more-variables-to-multiple-regression)**\n",
    "\n",
    "**Also we need to check if the variables that were fit in the model are independent of each other to avoid multicollinearity(so that the effect of each on the regression model becomes less precise.**\n",
    "\n",
    "**While it is tempting to include as many input variables as possible, this can dilute true associations and lead to large standard errors with wide and imprecise confidence intervals, or, conversely, identify spurious associations. The conventional technique is to first run the univariate analyses (i.e., relation of the outcome with each predictor, one at a time) and then use only those variables which meet a preset cutoff for significance to run a multivariable model and to not overcomplicate the model.\n",
    "[Resource link](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5543767/)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading countries df \n",
    "countries_df = pd.read_csv('./countries.csv')\n",
    "# defining a new df with combing the countries df\n",
    "df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "      <th>intercept</th>\n",
       "      <th>ab_page</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>834778</th>\n",
       "      <td>UK</td>\n",
       "      <td>2017-01-14 23:08:43.304998</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>928468</th>\n",
       "      <td>US</td>\n",
       "      <td>2017-01-23 14:44:16.387854</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822059</th>\n",
       "      <td>UK</td>\n",
       "      <td>2017-01-16 14:04:14.719771</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711597</th>\n",
       "      <td>UK</td>\n",
       "      <td>2017-01-22 03:14:24.763511</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710616</th>\n",
       "      <td>UK</td>\n",
       "      <td>2017-01-16 13:14:44.000513</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        country                   timestamp      group landing_page  \\\n",
       "user_id                                                               \n",
       "834778       UK  2017-01-14 23:08:43.304998    control     old_page   \n",
       "928468       US  2017-01-23 14:44:16.387854  treatment     new_page   \n",
       "822059       UK  2017-01-16 14:04:14.719771  treatment     new_page   \n",
       "711597       UK  2017-01-22 03:14:24.763511    control     old_page   \n",
       "710616       UK  2017-01-16 13:14:44.000513  treatment     new_page   \n",
       "\n",
       "         converted  intercept  ab_page  \n",
       "user_id                                 \n",
       "834778           0          1        0  \n",
       "928468           0          1        1  \n",
       "822059           1          1        1  \n",
       "711597           0          1        0  \n",
       "710616           0          1        1  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#printing the 1st 5 rows of df_new\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['UK', 'US', 'CA'], dtype=object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the unique values in (country) column\n",
    "df_new.country.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "      <th>intercept</th>\n",
       "      <th>ab_page</th>\n",
       "      <th>CA</th>\n",
       "      <th>UK</th>\n",
       "      <th>US</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>834778</th>\n",
       "      <td>UK</td>\n",
       "      <td>2017-01-14 23:08:43.304998</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>928468</th>\n",
       "      <td>US</td>\n",
       "      <td>2017-01-23 14:44:16.387854</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822059</th>\n",
       "      <td>UK</td>\n",
       "      <td>2017-01-16 14:04:14.719771</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711597</th>\n",
       "      <td>UK</td>\n",
       "      <td>2017-01-22 03:14:24.763511</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710616</th>\n",
       "      <td>UK</td>\n",
       "      <td>2017-01-16 13:14:44.000513</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        country                   timestamp      group landing_page  \\\n",
       "user_id                                                               \n",
       "834778       UK  2017-01-14 23:08:43.304998    control     old_page   \n",
       "928468       US  2017-01-23 14:44:16.387854  treatment     new_page   \n",
       "822059       UK  2017-01-16 14:04:14.719771  treatment     new_page   \n",
       "711597       UK  2017-01-22 03:14:24.763511    control     old_page   \n",
       "710616       UK  2017-01-16 13:14:44.000513  treatment     new_page   \n",
       "\n",
       "         converted  intercept  ab_page  CA  UK  US  \n",
       "user_id                                             \n",
       "834778           0          1        0   0   1   0  \n",
       "928468           0          1        1   0   0   1  \n",
       "822059           1          1        1   0   1   0  \n",
       "711597           0          1        0   0   1   0  \n",
       "710616           0          1        1   0   1   0  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Creating the necessary dummy variables\n",
    "df_new[['CA','UK', 'US']] = pd.get_dummies(df_new['country'])\n",
    "# Printing 1st 5 rows in the new df \n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366116\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>converted</td>    <th>  No. Observations:  </th>   <td>290584</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td>290581</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>     2</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Wed, 13 Jan 2021</td> <th>  Pseudo R-squ.:     </th>  <td>1.521e-05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>22:53:05</td>     <th>  Log-Likelihood:    </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>   <td>0.1984</td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>   -1.9967</td> <td>    0.007</td> <td> -292.314</td> <td> 0.000</td> <td>   -2.010</td> <td>   -1.983</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UK</th>        <td>    0.0099</td> <td>    0.013</td> <td>    0.746</td> <td> 0.456</td> <td>   -0.016</td> <td>    0.036</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CA</th>        <td>   -0.0408</td> <td>    0.027</td> <td>   -1.518</td> <td> 0.129</td> <td>   -0.093</td> <td>    0.012</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:              converted   No. Observations:               290584\n",
       "Model:                          Logit   Df Residuals:                   290581\n",
       "Method:                           MLE   Df Model:                            2\n",
       "Date:                Wed, 13 Jan 2021   Pseudo R-squ.:               1.521e-05\n",
       "Time:                        22:53:05   Log-Likelihood:            -1.0639e+05\n",
       "converged:                       True   LL-Null:                   -1.0639e+05\n",
       "Covariance Type:            nonrobust   LLR p-value:                    0.1984\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept     -1.9967      0.007   -292.314      0.000      -2.010      -1.983\n",
       "UK             0.0099      0.013      0.746      0.456      -0.016       0.036\n",
       "CA            -0.0408      0.027     -1.518      0.129      -0.093       0.012\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# applying the logistic regression model on countries exploratory variables setting US as the baseline\n",
    "logit_mod = sm.Logit(df_new['converted'],df_new[['intercept','UK','CA']])\n",
    "results2 = logit_mod.fit()\n",
    "results2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intercept    0.135779\n",
       "UK           1.009966\n",
       "CA           0.960018\n",
       "dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exponentiating the results summary to interpret the values\n",
    "np.exp(results2.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:**\n",
    "\n",
    "1- If an individual was from UK , he/she is 1.0099 more likely to make a conversion than if he/she was from the US, holding all other variables constant.\n",
    "\n",
    "2-If an individual was from CA , he/she is 0.96 more likely to make a conversion than if he/she was from the US, holding all other variables constant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h. Though you have now looked at the individual factors of country and page on conversion, we would now like to look at an interaction between page and country to see if there significant effects on conversion.  Create the necessary additional columns, and fit the new model.  \n",
    "\n",
    "Provide the summary results, and conclusions based on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366113\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>converted</td>    <th>  No. Observations:  </th>   <td>290584</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td>290580</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>     3</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Wed, 13 Jan 2021</td> <th>  Pseudo R-squ.:     </th>  <td>2.323e-05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>22:53:37</td>     <th>  Log-Likelihood:    </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>   <td>0.1760</td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>   -1.9893</td> <td>    0.009</td> <td> -223.763</td> <td> 0.000</td> <td>   -2.007</td> <td>   -1.972</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UK</th>        <td>    0.0099</td> <td>    0.013</td> <td>    0.743</td> <td> 0.457</td> <td>   -0.016</td> <td>    0.036</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CA</th>        <td>   -0.0408</td> <td>    0.027</td> <td>   -1.516</td> <td> 0.130</td> <td>   -0.093</td> <td>    0.012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ab_page</th>   <td>   -0.0149</td> <td>    0.011</td> <td>   -1.307</td> <td> 0.191</td> <td>   -0.037</td> <td>    0.007</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:              converted   No. Observations:               290584\n",
       "Model:                          Logit   Df Residuals:                   290580\n",
       "Method:                           MLE   Df Model:                            3\n",
       "Date:                Wed, 13 Jan 2021   Pseudo R-squ.:               2.323e-05\n",
       "Time:                        22:53:37   Log-Likelihood:            -1.0639e+05\n",
       "converged:                       True   LL-Null:                   -1.0639e+05\n",
       "Covariance Type:            nonrobust   LLR p-value:                    0.1760\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept     -1.9893      0.009   -223.763      0.000      -2.007      -1.972\n",
       "UK             0.0099      0.013      0.743      0.457      -0.016       0.036\n",
       "CA            -0.0408      0.027     -1.516      0.130      -0.093       0.012\n",
       "ab_page       -0.0149      0.011     -1.307      0.191      -0.037       0.007\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Fit Your Linear Model And Obtain the Results\n",
    "logit_mod = sm.Logit(df_new['converted'],df_new[['intercept','UK','CA','ab_page']])\n",
    "results3 = logit_mod.fit()\n",
    "results3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intercept    0.136795\n",
       "UK           1.009932\n",
       "CA           0.960062\n",
       "ab_page      0.985168\n",
       "dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exponentiating the results summary to interpret the values\n",
    "np.exp(results3.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:**\n",
    "\n",
    "1-if an individual uses the new test page, he/she is 0.985 more likely to make a conversion than using the old page, holding all other variables constant.\n",
    "\n",
    "2- If an individual is from UK , he/she is 1.0099 more likely to make a conversion than if he was from the US, holding all other variables constant.\n",
    "\n",
    "3.If an individual is from CA , he/she is 0.96 more likely to make a conversion than if he was from the US, holding all other variables constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing interaction between page and country to see if there significant effects on conversion\n",
    "# Creating Higher order variables\n",
    "df_new['US_ab_page'] = df_new['US'] * df_new['ab_page']\n",
    "df_new['CA_ab_page'] = df_new['CA'] * df_new['ab_page']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366109\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>converted</td>    <th>  No. Observations:  </th>   <td>290584</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td>290578</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>     5</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Wed, 13 Jan 2021</td> <th>  Pseudo R-squ.:     </th>  <td>3.482e-05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>23:05:26</td>     <th>  Log-Likelihood:    </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>   <td>0.1920</td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th>  <td>   -1.9922</td> <td>    0.016</td> <td> -123.457</td> <td> 0.000</td> <td>   -2.024</td> <td>   -1.961</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>US_ab_page</th> <td>   -0.0314</td> <td>    0.027</td> <td>   -1.181</td> <td> 0.238</td> <td>   -0.084</td> <td>    0.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CA_ab_page</th> <td>   -0.0783</td> <td>    0.057</td> <td>   -1.378</td> <td> 0.168</td> <td>   -0.190</td> <td>    0.033</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>US</th>         <td>    0.0057</td> <td>    0.019</td> <td>    0.306</td> <td> 0.760</td> <td>   -0.031</td> <td>    0.043</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CA</th>         <td>   -0.0118</td> <td>    0.040</td> <td>   -0.296</td> <td> 0.767</td> <td>   -0.090</td> <td>    0.066</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ab_page</th>    <td>    0.0108</td> <td>    0.023</td> <td>    0.475</td> <td> 0.635</td> <td>   -0.034</td> <td>    0.056</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:              converted   No. Observations:               290584\n",
       "Model:                          Logit   Df Residuals:                   290578\n",
       "Method:                           MLE   Df Model:                            5\n",
       "Date:                Wed, 13 Jan 2021   Pseudo R-squ.:               3.482e-05\n",
       "Time:                        23:05:26   Log-Likelihood:            -1.0639e+05\n",
       "converged:                       True   LL-Null:                   -1.0639e+05\n",
       "Covariance Type:            nonrobust   LLR p-value:                    0.1920\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept     -1.9922      0.016   -123.457      0.000      -2.024      -1.961\n",
       "US_ab_page    -0.0314      0.027     -1.181      0.238      -0.084       0.021\n",
       "CA_ab_page    -0.0783      0.057     -1.378      0.168      -0.190       0.033\n",
       "US             0.0057      0.019      0.306      0.760      -0.031       0.043\n",
       "CA            -0.0118      0.040     -0.296      0.767      -0.090       0.066\n",
       "ab_page        0.0108      0.023      0.475      0.635      -0.034       0.056\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Fit logistic regression Model with the new columns of interactions\n",
    "logit_mod = sm.Logit(df_new['converted'],df_new[['intercept', 'US_ab_page', 'CA_ab_page','US','CA','ab_page']])\n",
    "results4 = logit_mod.fit()\n",
    "results4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intercept     0.136392\n",
       "US_ab_page    0.969090\n",
       "CA_ab_page    0.924703\n",
       "US            1.005761\n",
       "CA            0.988285\n",
       "ab_page       1.010893\n",
       "dtype: float64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exponentiating the results summary to interpret the values\n",
    "np.exp(results4.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interactions between page and countries  don't seem to make a significant impact on the results just as interpreted before, differences remain statistically insignificant, and we can't reject the Null hypothesis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusions'></a>\n",
    "## Conclusions\n",
    "\n",
    "After analyzing the results of the 3 methods (probability, A/B testing and Logistic Regression model) to help the company understand the impact of deciding whether or not to proceed with the new page launching.\n",
    "the three methods' results were consistent and suggested the the old page performance is better than the new page with no  statistical evidence to reject the null hypothesis.\n",
    "\n",
    "However, in the real-world of online business, there are limitations we need to work with. In A/B testing we are limited by the time, resources and users we are happy to commit to any given test. There are some good practices that should be followed irrespective of what the sample sizes allow. For example, analyzing (sequential) tests on a weekly basis and not running tests for less than a week. For many businesses Tuesdays are not the same as Sundays, and even if you can run a satisfactory test in 3 days, you better plan it for a full seven days. Other best-practice advice can be found in white papers and books written by experts in the field.[Resource Link](http://blog.analytics-toolkit.com/2017/statistical-significance-ab-testing-complete-guide/)\n",
    "\n",
    "An advice here for best A/B testing practice in this [link](http://blog.analytics-toolkit.com/2017/statistical-significance-ab-testing-complete-guide/):\n",
    "\"My advice is to weigh the costs for designing, preparing and running the A/B test against the potential benefits (with a reasonable future extrapolation, e.g. several years) and see the sample sizes (and thus time) required by several different combinations of values for the three main parameters (significance, power, minimum effect size). Then, chose a statistical design that hits closest to the perfect balance. Using a flexible sequential testing framework such as AGILE can make the decision on the minimum effect size much easier, since in case the true difference is much larger or much smaller, the test will simply stop early so the efficiency sacrificed will be minimal\n",
    "While I don’t think there is a recipe for a perfect A/B test, I think the above advice for making a choice while facing several trade-offs is a helpful starting point for CRO and UX practitioners\".\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
